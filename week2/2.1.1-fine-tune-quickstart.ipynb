{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b4ceae-2d58-449d-a78e-742e587c68f9",
   "metadata": {},
   "source": [
    "<font size=\"5\">YelpReviewFull数据集微调</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31766172-025b-4fa7-8852-80f074dfc66e",
   "metadata": {},
   "source": [
    "yelp_review_full数据集：  \n",
    "Yelp（美国点评网站）的全量评论数据集，包含约 65 万条用户对商家的评论，每条评论带有 1-5 星的评分标签，常用于自然语言处理任务（如情感分析、文本分类等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e05aa-0e3d-4f27-9edd-d0f5eb804daf",
   "metadata": {},
   "source": [
    "<font size=\"4\">1 导入数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a92c4-5ef4-40c5-a1f8-20462f390647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:1087'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:1087'\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a317f57f-0aae-439b-96bc-834cb1fa03fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0b7ad-c88a-4c2a-af92-6a81475efea2",
   "metadata": {},
   "source": [
    "<font size=\"4\">2 预处理数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b09a80-df6b-4d87-b407-f0ec1a15bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97cf61fa-7ebb-4636-aa16-0b24cd40c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []  # 取出的索引值的列表\n",
    "    for _ in range(num_examples):  # 取num_examples个数\n",
    "        pick = random.randint(0, len(dataset) - 1)  # 在索引范围内取一个随机数\n",
    "        while pick in picks:  # 如果取的索引值之前取过，则重新取一个\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)  # 添加\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])  # 转化为df\n",
    "    for column, typ in dataset.features.items():  # dataset.features: {'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None), 'text': Value(dtype='string', id=None)}\n",
    "        if isinstance(typ, datasets.ClassLabel):  # 找到'label'列\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])  # 将label中的0-4的数字转化为'1-5 star'标签\n",
    "    display(HTML(df.to_html()))  # 打印\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5acf3c3-81c3-4c20-815d-3ffd001e4ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Three and a half stars.\\nNow we only ate here once, the food was really good and the service was great. We discussed our experience after the meal and honestly we have been more impressed with Doan and Ben Thanh so no I would NOT rate Lang Van the best hands down yada yada.\\n\\nBut this was only one single meal and the cooking was certainly tasty enough to be in the ballpark (if not in the lead). I wouldn't lose sleep over which one is \\\"the best\\\" in this case.  If you like Vietnamese then you'll enjoy dining at any of those three restaurants, we do.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Just had the BBQ on the terrace offered on Sundays during the summer.  It comes with an appetizer,salad, soup, shellfish buffet, an entree  with sides and dessert.  The food quality was fantastic (Kobe sliders, duck burgers, Olympia oysters, king crabs).  There are drink specials for $8. The cost is $49 which is expensive but I thought it was worth every penny.   The restaurant is beautiful and check out the bar.  Quite a change from the Marquessa.  We'll be back to try the regular menu soon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 star</td>\n",
       "      <td>My and my husband's contracts are up and we're torn between two phones right now.  I went here to check them out and ask my short list of questions.  \\n\\nThey took my name and it went in queue on their tv screen showing the next 5 people up for service.  This is a pretty good system - I like knowing where I am on the list and, in the meantime, I was able to play around with the phones.  \\n\\nWhile I was on deck, the employee at the door taking names asked me if I had decided on one yet.  Since he wasn't doing anything I asked him a couple of questions about the two phones I was interested in and also solicited his opinion.  He was glad to help.  We eventually asked our salesperson some of the same questions; he gave us totally different answers!  And the questions we got differing answers for were like \\\"what's the difference between the two in battery time?\\\" etc.  Uhhh, someone is telling us something wrong here and shouldn't every employee either know the product or find out the answer if they're not sure, instead of just giving us some bogus answer? \\n\\nI think I'll just use the store to browse the phones and I'll compare and order them online.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Great hotel. I like it how they don't have casino in the hotel. It's clean and more for family with kids. No crazy partying noise at the hotel which is good. One thing though, everytime we come back from somewhere by taxi, door men are \\\"checking in? checking in?\\\" even though we were staying for over a week (for business). That was annoying! It's such a nice hotel but that \\\"checking in? checking in?\\\" sounds so cheap. Otherwise, oh one more thing, house keeper threw away a bag of my personal item in a bathroom. We talked to a front desk and they said \\\"we never hear any negative comments like that\\\" that's it. So that was annoying but over all it's a great hotel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Crappy service. Came in to order one sandwich. Told me 30 min wait because they are in middle of a big order.  Employees goofing off while making sandwiches. Should have one person working counter orders.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Took a few clients here on business.  We had the tasting menu, and while the Foie Gras Custard 'Br\\u00fbl\\u00e9e' was one of the best things I've eaten, I was pretty underwhelmed by the rest of the dishes.  When you are dropping $200 (or more) per person on food and wine, I want to be blown away.  This place gets 3 stars for the fantastic service, though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I wouldn't give them any stars at all. The day we moved in the apartment was dirty carpet stained and scratches on the walls. We reported that to the office and they said that maintenance was going to come in to take care of that but they never showed up.  A week ago our neighbor caught his town home on fire, and ours suffered a lot of water and smoke damages. We didn't heard from the property manager until we contacted them to see what was the plan and when the out house was going to be ready for us to move back. So far it's been a week and our house is a complete disaster. we really like the home and the location but they lack customer service skills. We moved from Texas and we were not familiar with the area and didn't have a lot of time to find a place since my husbands job needed him here ASAP. I am pretty sure that we can find something much better for the same price... My advise is to look somewhere else!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Excellent service, items delivered on time.  The initial delivery was two chairs short, but with one phone call, the missing items were delivered promptly and with a smile.  Our guests thought our party decor looked great.  I wouldn't hesitate to use RSVP again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>just ok.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>This place is like a local Etsy. If you're looking for a gifts, house decor or other locally made goods, come here first. My favorite part of the store is the hand-picked vintage furniture. Such cute stuff all around and the prices are super reasonable. The owner is a warm and cheerful and has done an amazing job with this store. It's now my \\\"go to\\\" place for gifts (especially for babies) and household goods.  Stop by and check out this gem for yourself! And if you follow them on Instagram or Facebook you'll be the first to know about new arrivals.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbfe4a-3aa2-40ce-ad46-a1dd1f402d95",
   "metadata": {},
   "source": [
    "分词器：将原始文本转换为模型可理解的数字形式（即 \"token ID\"），同时处理文本的分割、标点符号等细节（符合模型的输入要求）。  \n",
    "bert-base-cased：预训练模型，\"bert\" 指 BERT 模型，\"base\" 表示基础版本（参数量较小），\"cased\" 表示该模型区分大小写（如 \"Apple\" 和 \"apple\" 会被视为不同 token）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f305b3d-76af-4172-b33a-6597f8e1647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76d55710137400b8b4ebca9fe89fdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6bdbc77e254648a12f405612b818ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # 自动分词器，能根据预训练模型的名称自动加载对应的分词器（不同模型的分词规则不同）\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    examples[\"text\"]：yelp_review_full数据集中的评论文本\n",
    "    padding=\"max_length\"：当文本长度不足时，补足到指定的最大长度\n",
    "    max_length=512：指定文本的最大长度为 512（BERT 模型设计的最大输入序列长度就是 512，超过会导致模型无法处理）\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)  # 将tokenize_function应用到整个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb94063-9fd1-46c0-aa46-b7fa5d5a5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset) # 数量不能超出数据集长度\n",
    "    picks = []  # 取出的索引值的列表\n",
    "    for _ in range(num_examples):  # 取num_examples个数\n",
    "        pick = random.randint(0, len(dataset) - 1)  # 在索引范围内取一个随机数\n",
    "        while pick in picks:  # 如果取的索引值之前取过，则重新取一个\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)  # 添加\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])  # 转化为df\n",
    "    for column, typ in dataset.features.items():  # 其中dataset.features: {'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None), 'text': Value(dtype='string', id=None)}\n",
    "        if isinstance(typ, datasets.ClassLabel):  # 找到'label'列\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])  # 将label中的0-4的数字转化为'1-5 star'标签\n",
    "    display(HTML(df.to_html()))  # 打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559899fb-1ba2-4623-9015-7d53863bb078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Hertz fails in their customer service. As Club Gold members, we initially asked one of the attendants where we should wait in line. He noted that the Club Gold line was closed, and told us to wait in the general line. After 30 minutes, we were served, only to be moved to the Club Gold line upstairs, who then told us to come back down and wait even more. We waited in three lines to get our car!! At the end of it, they gave us a car where the windows aren't even automatic. I will be sure to take my business elsewhere next time, as Hertz does not understand customer service.</td>\n",
       "      <td>[101, 1430, 5745, 12169, 1107, 1147, 8132, 1555, 119, 1249, 1998, 3487, 1484, 117, 1195, 2786, 1455, 1141, 1104, 1103, 19389, 1116, 1187, 1195, 1431, 3074, 1107, 1413, 119, 1124, 2382, 1115, 1103, 1998, 3487, 1413, 1108, 1804, 117, 1105, 1500, 1366, 1106, 3074, 1107, 1103, 1704, 1413, 119, 1258, 1476, 1904, 117, 1195, 1127, 1462, 117, 1178, 1106, 1129, 1427, 1106, 1103, 1998, 3487, 1413, 8829, 117, 1150, 1173, 1500, 1366, 1106, 1435, 1171, 1205, 1105, 3074, 1256, 1167, 119, 1284, 3932, 1107, 1210, 2442, 1106, 1243, 1412, 1610, 106, 106, 1335, 1103, 1322, 1104, 1122, 117, 1152, 1522, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)  # 展示元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d19ea-79e4-4c86-9857-0d58c31bfa0f",
   "metadata": {},
   "source": [
    "token_type_ids（token类型标识）：主要用于区分输入序列中的 “不同句子片段”， 例如：[CLS] I like cats [SEP] Do you like dogs? [SEP] 转化后为：[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]  \n",
    "attention_mask（注意力掩码）：告诉模型哪些 token 是 “真实有效的文本”标志1，哪些是 “填充的占位符”（padding）标志0，避免模型对无效的填充符号进行关注"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b836553-1dea-4094-8309-499805bffe3e",
   "metadata": {},
   "source": [
    "<font size=\"4\">3 部分数据微调模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84698e8-43c3-4a65-980a-b3fc8dd5e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5) (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "train_val = tokenized_datasets[\"train\"].shuffle(seed=42)  # shuffle：对训练集进行随机打乱；seed=42：随机种子，确保每次运行代码时打乱的顺序完全一致\n",
    "train_dataset = train_val.select(range(20000))  # 选择前2w条数据作为训练集，用于模型训练\n",
    "eval_dataset = train_val.select(range(20000, 22000))  # 选择2k数据作为验证集，用于在训练过程中评估模型性能（如验证准确率、损失等），帮助调整超参数或判断是否过拟合\n",
    "print(train_dataset.shape, eval_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1bf4c1-d2f7-4e85-b04a-5e3e859f61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-1): 2 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)  \n",
    "\"\"\"\n",
    "AutoModelForSequenceClassification: 专门用于序列分类任务（如文本分类、情感分析、主题识别等）的自动模型加载工具\n",
    "from_pretrained: 加载预训练模型，并初始化分类头\n",
    "num_labels=5：指定分类任务的类别数量为 5, yelp_review_full数据集的评论标签是1-5 star，共5个类别\n",
    "\"\"\"\n",
    "print(model.bert.encoder.layer[:2]) \n",
    "\"\"\"\n",
    "model：整个序列分类模型，包含两部分:基础 BERT 模型（model.bert）和分类头（model.classifier）\n",
    "model.bert：即基础BERT模型，核心是一个Transformer 编码器（model.bert.encoder）\n",
    "model.bert.encoder.layer：BERT 的编码器由多个Transformer 层堆叠而成（bert-base-cased包含 12 层），这是一个存储所有层的列表\n",
    "[:2]：取列表的前 2 个元素，即打印前两层 Transformer 层\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49b106e-8a3c-4328-acdc-4b7fe234c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\"\"\"\n",
    "TrainingArguments：用于定义模型训练的各种超参数和配置（如训练轮数、学习率、批量大小、日志保存路径等）\n",
    "Trainer：基于TrainingArguments的高级训练接口，封装了模型训练、验证、保存等完整流程，无需手动编写训练循环（如前向传播、反向传播、参数更新等），简化训练代码\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import evaluate  # 用于模型评估的工具库,内置了多种常见评估指标（如准确率、F1 值、BLEU 等），支持加载自定义指标，方便在训练过程中实时计算模型性能\n",
    "\n",
    "metric = evaluate.load(\"/home/cc/projects/llm-quickstart/LLM-quickstart-main/mywork/week2/evaluate-main/metrics/accuracy\")  # accuracy: 预测正确的样本数/总样本数\n",
    "\n",
    "model_dir = \"/home/cc/models/finetuned-models/bert-base-cased-finetune-yelp-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e8bf2a-698a-4a04-93da-759c0b1b7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 元组拆包\n",
    "    predictions = np.argmax(logits, axis=-1)  # 获取最大的\n",
    "    return metric.compute(predictions=predictions, references=labels)  # 计算 “预测正确的样本数 / 总样本数”，返回一个字典（例如{\"accuracy\": 0.60}）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12afdc4e-2b86-4f03-91e5-7c91a3d3540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=2,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=accuracy,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/home/cc/models/finetuned-models/bert-base-cased-finetune-yelp-2,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/cc/models/finetuned-models/bert-base-cased-finetune-yelp-2,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# 将递归深度从默认~1000提高到10000（足够覆盖BERT的层级）\n",
    "sys.setrecursionlimit(10000)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,  # 模型输出路径\n",
    "    per_device_train_batch_size=16,  # 单卡训练批次大小\n",
    "    per_device_eval_batch_size=32,  # 单卡评估批次大小\n",
    "    gradient_accumulation_steps=2,  # 梯度累积步骤（总批大小 = 16 * 2 * 2卡 = 64）\n",
    "    num_train_epochs=5,  # 训练轮数\n",
    "    learning_rate=5e-5,  # 学习率\n",
    "    weight_decay=0.01,  # 权重衰减（L2正则化）\n",
    "    warmup_ratio=0.05,  # 预热比例（前5%训练步长）\n",
    "    lr_scheduler_type=\"linear\",  # 学习率调度器类型\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=500,  # 每500步记录日志\n",
    "    evaluation_strategy=\"epoch\",  # 每个epoch结束评估\n",
    "    save_strategy=\"epoch\",  # 每个epoch结束保存模型\n",
    "    save_total_limit=2,  # 最多保留2个检查点\n",
    "    load_best_model_at_end=True,  # 训练结束时加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",  # 根据准确率选择最佳模型\n",
    "    greater_is_better=True,  # 准确率越高越好\n",
    "    report_to=\"tensorboard\",  # 使用TensorBoard记录\n",
    "    fp16=True,  # 启用混合精度训练（利用RTX 4090的Tensor Core）\n",
    "    dataloader_num_workers=4,  # 数据加载线程数\n",
    "    remove_unused_columns=False,  # # 禁用自动移除未用列（确保保留所有特征）\n",
    "    gradient_checkpointing=False,  # 梯度检查点（节省显存）\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,  # 指定要训练的模型\n",
    "    args=training_args,  # 传入训练配置参数\n",
    "    train_dataset=train_dataset,  # 训练集\n",
    "    eval_dataset=eval_dataset,  # 验证集\n",
    "    compute_metrics=compute_metrics,  # 评估指标计算函数，也就是accuracy\n",
    ")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef7d5fed-d0a7-4e5b-af8d-242678b70fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1560 17:59, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.904373</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.969500</td>\n",
       "      <td>1.053166</td>\n",
       "      <td>0.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>1.397879</td>\n",
       "      <td>0.607000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1560, training_loss=0.5845682676021869, metrics={'train_runtime': 1080.2132, 'train_samples_per_second': 92.574, 'train_steps_per_second': 1.444, 'total_flos': 2.626971534360576e+16, 'train_loss': 0.5845682676021869, 'epoch': 4.99})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e3f05-c4b5-49f5-84df-cc1aa7b3a5c3",
   "metadata": {},
   "source": [
    "指标解释：    \n",
    "Epoch：训练轮次，指模型完整遍历训练数据集的次数。例如，Epoch 0表示第 1 轮训练（部分框架从 0 开始计数），Epoch 4表示第 5 轮训练  \n",
    "Training Loss（训练损失）：模型在训练集上的损失值（通常是交叉熵损失，用于衡量模型预测与训练数据真实标签的差距）。数值越小，说明模型在训练集上的拟合效果越好  \n",
    "Validation Loss（验证损失）：模型在验证集上的损失值，用于衡量模型对未见过的数据（验证集）的预测误差。数值越小，说明模型泛化能力越好  \n",
    "Accuracy（准确率）：模型在验证集上的预测准确率（正确预测的样本数 / 总样本数），直接反映模型的分类性能。数值越高，模型效果越好  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208512d-3ea5-4ac1-99e3-a8357114d5eb",
   "metadata": {},
   "source": [
    "三轮准确率分别为0.606（60.6%）、0.604（60.4%）、0.607（60.7%），整体波动极小，几乎没有提升  \n",
    "尽管训练损失在下降，但模型在验证集上的实际分类性能没有改善，始终维持在 60% 左右。结合验证损失上升的趋势，说明模型并未学到真正能泛化的规律，而是在训练集上 “过拟合”，导致在验证集上无法有效预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700a787-6468-415b-b2e1-9bdd261753c8",
   "metadata": {},
   "source": [
    "TrainOutput解释：  \n",
    "global_step=1560：总的训练步数（参数更新次数），步数 = 训练样本数 ÷ 每步处理的样本数 × 训练轮次  \n",
    "training_loss=0.5845682676021869：整个训练过程的平均训练损失（在所有训练 step 上的平均），这个值较低（0.58），说明模型在训练集上的拟合效果较好，对训练数据的 “记忆” 程度较高。  \n",
    "metrics：   \n",
    "      train_runtime=1080.2132：总训练时间，单位为秒（约 18 分钟）  \n",
    "      train_samples_per_second=92.574：每秒处理的训练样本数  \n",
    "      train_steps_per_second=1.444：每秒完成的训练步数  \n",
    "      total_flos=2.626971534360576e+16：训练过程中总的浮点运算次数（FLOPs）  \n",
    "      train_loss=0.5845682676021869：与前面的training_loss一致，即平均训练损失  \n",
    "      epoch=4.99：实际完成的训练轮次（接近 5 轮），与预设的训练轮次基本吻合（可能因最后一轮未完全处理完所有样本而略小于 5）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "339c41d7-dfd5-4b5c-819f-85b054b6e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))  # 选100个元素作为测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8e9cbc-e81e-40fd-a76c-961f79578979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9968435764312744,\n",
       " 'eval_accuracy': 0.52,\n",
       " 'eval_runtime': 0.5526,\n",
       " 'eval_samples_per_second': 180.966,\n",
       " 'eval_steps_per_second': 3.619,\n",
       " 'epoch': 4.99}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)  # 评估在测试集上的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db0007-5a10-4a18-8983-126267667612",
   "metadata": {},
   "source": [
    "eval_loss': 0.9968：测试集上的损失值，与之前验证集最终评估的损失接近（验证集约 0.997），说明模型在测试集和验证集上的预测误差水平相当   \n",
    "eval_accuracy': 0.52：测试集上的准确率为 52%，与验证集最终准确率一致，整体表现一般，未能达到理想的泛化效果  \n",
    "eval_runtime、eval_samples_per_second：评估效率指标，0.55 秒完成，每秒处理 181 个样本 \n",
    "epoch': 4.99：基于接近5轮训练后的权重 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a35b-28dd-481b-8ee7-1a5015cc0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)  # 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74e1f93c-9e1f-4dbe-be98-b191a84c3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3ebf7-b74a-4a3f-9885-bf249dcd9173",
   "metadata": {},
   "source": [
    "<font size=\"4\">4 全部数据微调模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd365694-be48-406e-97be-394d57a9bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585000, 5) (65000, 5)\n"
     ]
    }
   ],
   "source": [
    "train_val = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "split_index = int(0.9 * len(train_val))  # 90% 作为训练集，10% 作为验证集\n",
    "train_dataset = train_val.select(range(split_index))\n",
    "eval_dataset = train_val.select(range(split_index, len(train_val)))\n",
    "print(train_dataset.shape, eval_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f59467-f366-4103-ade2-ec4c5b46d4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45705' max='45705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45705/45705 8:31:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>0.729362</td>\n",
       "      <td>0.679092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.712976</td>\n",
       "      <td>0.692523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>0.736168</td>\n",
       "      <td>0.692631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.812186</td>\n",
       "      <td>0.689308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.924588</td>\n",
       "      <td>0.684877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45705, training_loss=0.5784050744806516, metrics={'train_runtime': 30702.4255, 'train_samples_per_second': 95.269, 'train_steps_per_second': 1.489, 'total_flos': 7.696205667072e+17, 'train_loss': 0.5784050744806516, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0828efe-ade7-49b4-a123-d68b8507802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce25f69-a2a5-4bd2-8d44-5a9c4f6875ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.virtualenvs/peft/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7708825469017029,\n",
       " 'eval_accuracy': 0.671,\n",
       " 'eval_runtime': 3.4172,\n",
       " 'eval_samples_per_second': 292.64,\n",
       " 'eval_steps_per_second': 4.682,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
